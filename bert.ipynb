{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to tuto: https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import transformers, datasets\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "dataset_path =  'D://data_phd_code_from_scatch//datasets//cornell movie-dialogs corpus'\n",
    "### loading all data into memory\n",
    "corpus_movie_conv = dataset_path +'/movie_conversations.txt'\n",
    "corpus_movie_lines = dataset_path +'/movie_lines.txt'\n",
    "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
    "    conv = c.readlines()\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
    "    lines = l.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### splitting text using special lines\n",
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]\n",
    "\n",
    "### generate question answer pairs\n",
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        \n",
    "        if i == len(ids) - 1:\n",
    "            break\n",
    "\n",
    "        first = lines_dic[ids[i]].strip()  \n",
    "        second = lines_dic[ids[i+1]].strip() \n",
    "\n",
    "        qa_pairs.append(' '.join(first.split()[:MAX_LEN]))\n",
    "        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece tokenization\n",
    "### save data as txt file\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "save_path_data_processed = 'D:/data_phd_code_from_scatch/datasets/data_processed/movie-dialogs'\n",
    "for sample in tqdm.tqdm([x[0] for x in pairs]):\n",
    "    text_data.append(sample)\n",
    "    # once we hit the 10K mark, save to file\n",
    "    if len(text_data) == 10000:\n",
    "        with open(save_path_data_processed + '/text_'+ str(file_count) +'.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "\n",
    "# List to hold the paths of .txt files\n",
    "txt_files = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(save_path_data_processed):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            txt_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer_train\n",
    "tokenizer = tokenizer_train.train_bert_tokenizer(txt_files, 'bert_toy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "vocab_file_path = 'bert_tokenizer/bert_toy-vocab.txt'\n",
    "tokenizer = BertTokenizer(vocab_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(\"Hello, how are you?\", add_special_tokens=True)\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "\n",
    "print(f\"Encoded Text: {encoded_text}\")\n",
    "print(f\"Decoded Text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_prapare import BERTDataset\n",
    "train_data = BERTDataset(pairs, tokenizer, seq_len=64)\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert_input': tensor([    1,     3,     3,     3,     3,   417,     3,     3,   146, 10230,\n",
      "            3,   153,   300,   179,    17,    17,    17,     2,   182,    34,\n",
      "           17,    17,    17,     2,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]), 'bert_label': tensor([   0,  255,   11,   58,  220,    0,  172,  491,    0,  230, 1231,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]), 'segment_label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'is_next': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(train_loader))\n",
    "print(train_data[random.randrange(len(train_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERT, BERTLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTTrainer\n\u001b[1;32m----> 4\u001b[0m train_data \u001b[38;5;241m=\u001b[39m BERTDataset(\u001b[43mpairs\u001b[49m, seq_len\u001b[38;5;241m=\u001b[39mMAX_LEN, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m      6\u001b[0m    train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m BERT(\n\u001b[0;32m      9\u001b[0m   vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mvocab),\n\u001b[0;32m     10\u001b[0m   d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m   dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "from dataset_prapare import BERTDataset\n",
    "from models import BERT, BERTLM\n",
    "from trainers import BERTTrainer\n",
    "train_data = BERTDataset(pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "bert_model = BERT(\n",
    "  vocab_size=len(tokenizer.vocab),\n",
    "  d_model=768,\n",
    "  n_layers=2,\n",
    "  heads=12,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
    "\n",
    "bert_trainer = BERTTrainer(bert_lm, train_loader, device='cpu')\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  bert_trainer.train(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_input': tensor([[    1,  3902,    17,  ...,     0,     0,     0],\n",
       "         [    1,   302,   234,  ...,     0,     0,     0],\n",
       "         [    1,     3,   308,  ...,    15,   395,    17],\n",
       "         ...,\n",
       "         [    1,   194,   255,  ...,     0,     0,     0],\n",
       "         [    1,   514,   153,  ...,     0,     0,     0],\n",
       "         [    1, 19782,   107,  ...,     0,     0,     0]]),\n",
       " 'bert_label': tensor([[  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0, 248,   0,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]]),\n",
       " 'segment_label': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 2, 2, 2],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'is_next': tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 1, 1])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
